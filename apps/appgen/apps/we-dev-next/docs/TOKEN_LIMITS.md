# Token Limits Configuration

Ce document explique comment configurer et utiliser les limites de tokens pour les g√©n√©rations AI dans l'application we-dev-next.

## Variables d'environnement

Ajoutez ces variables dans votre fichier `.env` :

```env
# AI Generation Token Limits
AI_MAX_OUTPUT_TOKENS=8192      # Nombre maximum de tokens g√©n√©r√©s par l'AI
AI_MAX_INPUT_TOKENS=128000     # Nombre maximum de tokens dans le contexte d'entr√©e
AI_STANDARD_TOKEN_LIMIT=128000 # Seuil pour basculer en mode token-limit√©
```

## Description des limites

### AI_MAX_OUTPUT_TOKENS

- **D√©faut**: `8192`
- **Description**: Nombre maximum de tokens que l'AI peut g√©n√©rer dans une seule r√©ponse
- **Utilisation**: Contr√¥le la longueur de la r√©ponse de l'AI
- **Recommandations**:
  - `4096` - Pour des r√©ponses courtes et rapides
  - `8192` - √âquilibre entre longueur et co√ªt (recommand√©)
  - `16384` - Pour des g√©n√©rations plus longues
  - `32768` - Pour des projets tr√®s complexes (co√ªteux)

### AI_MAX_INPUT_TOKENS

- **D√©faut**: `128000`
- **Description**: Nombre maximum de tokens autoris√©s dans le contexte d'entr√©e
- **Utilisation**: Limite la taille du contexte envoy√© √† l'AI
- **Recommandations**:
  - `32000` - Pour des projets simples
  - `128000` - Standard pour la plupart des projets (recommand√©)
  - `200000` - Pour des projets tr√®s larges (si support√© par le mod√®le)

### AI_STANDARD_TOKEN_LIMIT

- **D√©faut**: `128000`
- **Description**: Seuil de tokens pour activer le mode de gestion des tokens
- **Utilisation**: Quand le contenu d√©passe cette limite, le syst√®me active la gestion intelligente des tokens
- **Recommandations**: Devrait √™tre √©gal ou l√©g√®rement inf√©rieur √† `AI_MAX_INPUT_TOKENS`

## Comment √ßa fonctionne

### 1. Au d√©marrage du serveur

Le serveur affiche automatiquement les limites de tokens configur√©es :

```
üìä TOKEN LIMITS CONFIGURATION:
  Max Output Tokens: 8,192
  Max Input Tokens: 128,000
  Standard Token Limit: 128,000
```

### 2. Pendant la g√©n√©ration

Le syst√®me utilise ces limites pour :

1. **Contr√¥ler la longueur des r√©ponses** via `maxOutputTokens` dans la configuration du mod√®le
2. **G√©rer le contexte** en v√©rifiant si le contenu d√©passe `standardTokenLimit`
3. **Optimiser les prompts** en s√©lectionnant uniquement les fichiers pertinents si n√©cessaire

### 3. Gestion automatique

Quand le contexte d√©passe `AI_STANDARD_TOKEN_LIMIT` :

```typescript
if (tokenCount > tokenLimits.standardTokenLimit) {
  // Active le mode token-limit√©
  // S√©lectionne uniquement les fichiers pertinents
  // Utilise un prompt optimis√©
}
```

## Exemples de configuration

### Configuration √©conomique (co√ªts r√©duits)

```env
AI_MAX_OUTPUT_TOKENS=4096
AI_MAX_INPUT_TOKENS=32000
AI_STANDARD_TOKEN_LIMIT=32000
```

### Configuration standard (recommand√©e)

```env
AI_MAX_OUTPUT_TOKENS=8192
AI_MAX_INPUT_TOKENS=128000
AI_STANDARD_TOKEN_LIMIT=128000
```

### Configuration haute performance

```env
AI_MAX_OUTPUT_TOKENS=16384
AI_MAX_INPUT_TOKENS=200000
AI_STANDARD_TOKEN_LIMIT=180000
```

### Configuration maximale (projets tr√®s complexes)

```env
AI_MAX_OUTPUT_TOKENS=32768
AI_MAX_INPUT_TOKENS=200000
AI_STANDARD_TOKEN_LIMIT=180000
```

## Fichiers modifi√©s

Les limites de tokens sont utilis√©es dans :

1. **`src/config/tokenLimits.ts`** - Configuration centralis√©e
2. **`src/config/modelConfig.ts`** - Configuration des mod√®les AI
3. **`src/handlers/builderHandler.ts`** - Gestion des g√©n√©rations
4. **`src/server.ts`** - Affichage au d√©marrage

## Validation

Le syst√®me valide automatiquement les limites au d√©marrage :

- ‚úÖ V√©rifie que les valeurs sont positives
- ‚úÖ Avertit si `maxOutputTokens > maxInputTokens`
- ‚úÖ Affiche les valeurs configur√©es dans les logs

## Monitoring

Pour voir les limites actuelles, d√©marrez le serveur :

```bash
npm run dev
```

Les limites seront affich√©es dans la console au d√©marrage.

## D√©pannage

### Erreur: "Invalid AI_MAX_OUTPUT_TOKENS"

- V√©rifiez que la valeur est un nombre positif
- Exemple valide: `AI_MAX_OUTPUT_TOKENS=8192`

### Erreur: "Invalid AI_MAX_INPUT_TOKENS"

- V√©rifiez que la valeur est un nombre positif
- Exemple valide: `AI_MAX_INPUT_TOKENS=128000`

### Avertissement: "maxOutputTokens > maxInputTokens"

- L'AI ne peut pas g√©n√©rer plus de tokens qu'elle n'en re√ßoit
- Ajustez `AI_MAX_OUTPUT_TOKENS` pour qu'il soit inf√©rieur √† `AI_MAX_INPUT_TOKENS`

### R√©ponses tronqu√©es

- Augmentez `AI_MAX_OUTPUT_TOKENS`
- V√©rifiez les limites du mod√®le AI utilis√©

### Erreurs de contexte trop large

- R√©duisez `AI_MAX_INPUT_TOKENS`
- Le syst√®me activera automatiquement la gestion intelligente des tokens

## Bonnes pratiques

1. **Commencez avec les valeurs par d√©faut** et ajustez selon vos besoins
2. **Surveillez les co√ªts** - Plus de tokens = co√ªts plus √©lev√©s
3. **Testez diff√©rentes configurations** pour trouver le bon √©quilibre
4. **Documentez vos changements** dans votre fichier `.env`
5. **Utilisez des valeurs coh√©rentes** entre les environnements (dev/prod)

## Support des mod√®les

Diff√©rents mod√®les AI ont des limites diff√©rentes :

| Mod√®le          | Max Input   | Max Output | Recommandation            |
| --------------- | ----------- | ---------- | ------------------------- |
| Gemini 1.5 Pro  | 2M tokens   | 8K tokens  | AI_MAX_OUTPUT_TOKENS=8192 |
| GPT-4 Turbo     | 128K tokens | 4K tokens  | AI_MAX_OUTPUT_TOKENS=4096 |
| Claude 3 Sonnet | 200K tokens | 4K tokens  | AI_MAX_OUTPUT_TOKENS=4096 |
| DeepSeek        | 64K tokens  | 8K tokens  | AI_MAX_OUTPUT_TOKENS=8192 |

V√©rifiez toujours les limites du mod√®le que vous utilisez et configurez les variables en cons√©quence.
