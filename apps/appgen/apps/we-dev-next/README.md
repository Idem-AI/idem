# @we-dev/express

Express.js 5.2.1 replica of the we-dev-next application with complete feature parity.

## ğŸš€ Features

- âœ… **Complete API Routes**: All routes from Next.js replicated
  - `/api/chat` - AI chat with builder and chat modes
  - `/api/deploy` - Netlify deployment
  - `/api/enhancedPrompt` - AI prompt enhancement
  - `/api/model` - Model configuration management

- âœ… **AI Integration**: Full support for multiple AI providers
  - OpenAI (GPT-4, GPT-3.5)
  - Google Gemini
  - DeepSeek
  - Anthropic Claude

- âœ… **Project Generation**: Complete project prompt service
  - Landing page generation (separate, integrated, only)
  - Full application generation
  - Brand identity integration
  - Technology stack configuration
  - Use case diagram implementation

- âœ… **Advanced Features**:
  - Screenshot capture integration
  - File processing and diff generation
  - Token management
  - Streaming responses
  - Tool calling support
  - Docker configuration generation

## ğŸ“¦ Installation

```bash
# Install dependencies
npm install

# Copy environment variables
cp .env.example .env

# Configure your environment variables
# Edit .env with your API keys
```

## ğŸ”§ Configuration

Create a `.env` file with the following variables:

```env
# Server Configuration
PORT=3002
NODE_ENV=development

# AI API Configuration
THIRD_API_URL=https://api.openai.com/v1
THIRD_API_KEY=your_api_key_here

# AI Models Configuration (JSON format)
AI_MODELS_CONFIG=[{"modelName":"GPT-4","modelKey":"gpt-4","useImage":true,"provider":"openai","functionCall":true}]
AI_DEFAULT_MODEL=gpt-4

# Screenshot Service
SCREENSHOTONE_API_KEY=your_screenshotone_api_key

# Netlify Deployment
NETLIFY_TOKEN=your_netlify_token
NETLIFY_DEPLOY_URL=https://api.netlify.com/api/v1/sites

# CORS Configuration
CORS_ORIGIN=*

# AI Generation Token Limits
AI_MAX_OUTPUT_TOKENS=8192      # Maximum tokens in AI response
AI_MAX_INPUT_TOKENS=128000     # Maximum tokens in input context
AI_STANDARD_TOKEN_LIMIT=128000 # Threshold for token-limited mode
```

### Token Limits Configuration

Control AI generation token limits via environment variables. See [TOKEN_LIMITS.md](./TOKEN_LIMITS.md) for detailed documentation.

**Quick Configuration:**

- `AI_MAX_OUTPUT_TOKENS` - Maximum tokens the AI can generate (default: 8192)
- `AI_MAX_INPUT_TOKENS` - Maximum tokens in input context (default: 128000)
- `AI_STANDARD_TOKEN_LIMIT` - Threshold for smart token management (default: 128000)

**Example configurations:**

```env
# Standard (recommended)
AI_MAX_OUTPUT_TOKENS=8192
AI_MAX_INPUT_TOKENS=128000

# High performance
AI_MAX_OUTPUT_TOKENS=16384
AI_MAX_INPUT_TOKENS=200000

# Cost-effective
AI_MAX_OUTPUT_TOKENS=4096
AI_MAX_INPUT_TOKENS=32000
```

## ğŸƒ Running the Application

### Development Mode

```bash
npm run dev
```

### Production Build

```bash
npm run build
npm start
```

## ğŸ“¡ API Endpoints

### POST /api/chat

Chat with AI in builder or chat mode.

**Request Body:**

```json
{
  "messages": [
    {
      "id": "uuid",
      "role": "user",
      "content": "Create a todo app"
    }
  ],
  "model": "gpt-4",
  "mode": "builder",
  "projectData": {
    "name": "My Project",
    "description": "Project description",
    "type": "web",
    "analysisResultModel": {}
  }
}
```

**Headers:**

- `userId` (optional): User identifier for token tracking

### POST /api/deploy

Deploy a zip file to Netlify.

**Request:**

- Content-Type: `multipart/form-data`
- Field: `file` (zip file)

**Response:**

```json
{
  "success": true,
  "url": "https://your-app.netlify.app",
  "siteInfo": {}
}
```

### POST /api/enhancedPrompt

Enhance a prompt using AI.

**Request Body:**

```json
{
  "text": "Your prompt to enhance"
}
```

**Response:**

```json
{
  "code": 0,
  "text": "Enhanced prompt"
}
```

### GET /api/model

Get available AI models.

**Response:**

```json
[
  {
    "modelName": "GPT-4",
    "modelKey": "gpt-4",
    "useImage": true,
    "provider": "openai",
    "functionCall": true
  }
]
```

### GET /api/model/config

Get model configuration (same as /api/model).

### GET /api/model/default

Get default model.

**Response:**

```json
{
  "modelKey": "gpt-4"
}
```

### GET /health

Health check endpoint.

**Response:**

```json
{
  "status": "healthy",
  "timestamp": "2024-01-28T10:00:00.000Z",
  "uptime": 123.456
}
```

## ğŸ—ï¸ Project Structure

```
src/
â”œâ”€â”€ config/              # Configuration files
â”‚   â”œâ”€â”€ dockerfilePrompt.ts
â”‚   â”œâ”€â”€ modelConfig.ts
â”‚   â””â”€â”€ prompts.ts
â”œâ”€â”€ handlers/            # Request handlers
â”‚   â”œâ”€â”€ builderHandler.ts
â”‚   â””â”€â”€ chatHandler.ts
â”œâ”€â”€ middleware/          # Express middleware
â”‚   â”œâ”€â”€ cors.ts
â”‚   â””â”€â”€ errorHandler.ts
â”œâ”€â”€ routes/              # API routes
â”‚   â”œâ”€â”€ chat.ts
â”‚   â”œâ”€â”€ deploy.ts
â”‚   â”œâ”€â”€ enhancedPrompt.ts
â”‚   â””â”€â”€ model.ts
â”œâ”€â”€ services/            # Business logic services
â”‚   â”œâ”€â”€ aiService.ts
â”‚   â””â”€â”€ projectPromptService.ts
â”œâ”€â”€ types/               # TypeScript types
â”‚   â””â”€â”€ project.ts
â”œâ”€â”€ utils/               # Utility functions
â”‚   â”œâ”€â”€ diffGenerator.ts
â”‚   â”œâ”€â”€ fileProcessor.ts
â”‚   â”œâ”€â”€ fileTypeDetector.ts
â”‚   â”œâ”€â”€ json2zod.ts
â”‚   â”œâ”€â”€ logger.ts
â”‚   â”œâ”€â”€ markdown.ts
â”‚   â”œâ”€â”€ messageParser.ts
â”‚   â”œâ”€â”€ screenshotone.ts
â”‚   â”œâ”€â”€ streamResponse.ts
â”‚   â”œâ”€â”€ stripIndent.ts
â”‚   â”œâ”€â”€ switchableStream.ts
â”‚   â”œâ”€â”€ tokenHandler.ts
â”‚   â””â”€â”€ tokens.ts
â””â”€â”€ server.ts            # Main server file
```

## ğŸ”„ Differences from Next.js Version

### Architecture

- **Framework**: Express.js 5.2.1 instead of Next.js 14
- **Routing**: Traditional Express routing instead of Next.js App Router
- **Middleware**: Custom middleware instead of Next.js middleware
- **File Upload**: Multer instead of Next.js FormData

### Advantages

- âœ… Simpler deployment (no SSR complexity)
- âœ… More control over middleware
- âœ… Better performance for API-only workloads
- âœ… Easier to integrate with existing Express ecosystems
- âœ… Standard Node.js patterns

### Feature Parity

- âœ… All API endpoints replicated
- âœ… Same AI integration
- âœ… Same project generation logic
- âœ… Same prompt system
- âœ… Same file processing
- âœ… Same streaming responses
- âœ… Same error handling

## ğŸ§ª Testing

```bash
# Test health endpoint
curl http://localhost:3002/health

# Test chat endpoint
curl -X POST http://localhost:3002/api/chat \
  -H "Content-Type: application/json" \
  -H "userId: test-user" \
  -d '{
    "messages": [{"id":"1","role":"user","content":"Create a todo app"}],
    "model": "gpt-4",
    "mode": "builder"
  }'

# Test model endpoint
curl http://localhost:3002/api/model
```

## ğŸ“ Logging

The application uses a comprehensive logging system with:

- Structured logging with timestamps
- Log levels: INFO, WARN, ERROR, DEBUG, SUCCESS
- Step tracking for complex operations
- Detailed error information

## ğŸ”’ Security

- Helmet.js for security headers
- CORS configuration
- Request size limits (50mb)
- Environment variable protection
- Error sanitization in production

## ğŸš€ Deployment

### Docker

```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY dist ./dist
EXPOSE 3002
CMD ["node", "dist/server.js"]
```

### Build and Run

```bash
npm run build
docker build -t we-dev-express .
docker run -p 3002:3002 --env-file .env we-dev-express
```

## ğŸ“„ License

Same as parent project.

## ğŸ¤ Contributing

This is a replica of the Next.js version. Maintain feature parity when making changes.

## ğŸ“š Documentation

- [Token Limits Configuration](./TOKEN_LIMITS.md) - Configure AI generation token limits
- [Next.js Original](../we-dev-next/README.md)
- [API Documentation](./docs/API.md)
- [Architecture](./docs/ARCHITECTURE.md)

## ğŸ†˜ Support

For issues or questions, please refer to the main project documentation.
